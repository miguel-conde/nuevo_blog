[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "nuevo_blog",
    "section": "",
    "text": "Productividad y satisfacción laboral en la Economía del Conocimiento\n\n\n\n\n\n\ndata\n\n\nmanagement\n\n\ndeep work\n\n\n\n\n\n\n\n\n\nSep 23, 2024\n\n\nMiguel Conde\n\n\n\n\n\n\n\n\n\n\n\n\nPost With Code\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nSep 21, 2024\n\n\nHarlow Malloc\n\n\n\n\n\n\n\n\n\n\n\n\nEl incendio de la pradera\n\n\n\n\n\n\nstatistics\n\n\nmachine learning\n\n\n\n\n\n\n\n\n\nSep 18, 2024\n\n\nMiguel Conde\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nSep 18, 2024\n\n\nTristan O’Malley\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/dos_culturas/index.html",
    "href": "posts/dos_culturas/index.html",
    "title": "El incendio de la pradera",
    "section": "",
    "text": "En agosto de 2001 Leo Breiman incendió la pradera en la que las tribus de estadísticos plantaban tranquilamente sus tipees y se dedicaban desde hacía décadas a la caza del bisonte econométrico o clínico, con sus arcos de modelo lineal o logísitco, y al pacífico cultivo de las hipótesis nulas.\nUn incendio. Un terremoto. Un tsunami.\nPero ¿quién era Leo? Paradójicamente, Leo era uno de ellos. Al comienzo de su carrera había pasado 7 años como profesor e investigador en teoría de la probabilidad y estadística matemática. Pero entonces renunció a su vida académica y se convirtió en consultor independiente a tiempo completo. La experiencia lo transformó. Él no se dio cuenta de su cambio hasta que, tras 13 años, volvió al mundo académico en 1980. Y lo hizo nada menos que en en el Departamento de Estadística de la Univesidad de California, Berkeley.\nAllí tuvo que volver a sumergirse en la lectura de publicaciones de investigación estadística, como Annals of Statistics o Statistical Science. Se dio cuenta de que “la investigación distaba mucho de lo que había estado haciendo como consultor. Todos los artículos comienzan y terminan con modelos de datos”.\nComo consultor había tenido que enfrentarse a múltiples problemas en los que lo esencial era crear modelos que predijeran lo mejor posible: los niveles de ozono del día siguiente, el tipo de barco a partir de su huella radar o de un submarino por la de sonar, etc. Ahí afuera daban importancia a predecir.\n¿Qué entendía por modelos de datos? Lo que nosotros vemos como modelos estadísticos clásicos: modelos lineales, logísticos, LDA (Linear Discriminant Analysis)… Se había dado cuenta de que no funcionaban tan bien como sería deseable en muchos problemas de predicción.\nAsí que empezó a jugar con árboles. Eureka. La alternativa a los modelos de datos eran los modelos algorítmicos.\nY aquí volvemos al incendio, al terremoto y al tsunami: decidió compartir su “Eureka” con toda la tribu y lo hizo en una de las prestigiosas revistas profesionales que tanto le habían dado que pensar. Publicó “Statistical Modeling: The Two Cultures”.\nLo petó.\nEn este trabajo, Breiman criticaba duramente la orientación predominante en la estadística hacia el uso exclusivo de modelos probabilísticos y planteaba que la comunidad estaba ignorando una realidad emergente: el poder de los métodos algorítmicos. Según Breiman, estos métodos, como los árboles de decisión, bagging y random forests, ofrecían una capacidad predictiva superior en muchos casos prácticos, pero eran desestimados por la mayoría de los estadísticos debido a su falta de interpretabilidad o su alejamiento de los paradigmas tradicionales.\nBreiman argumentaba que había dos culturas en la estadística: una centrada en la inferencia a partir de modelos matemáticos basados en suposiciones probabilísticas (la escuela tradicional) y otra, la cultura algorítmica, enfocada en la precisión predictiva mediante el uso de modelos flexibles capaces de ajustarse a grandes volúmenes de datos sin estar tan limitados por supuestos rígidos. Esta confrontación desencadenó un debate en la comunidad estadística, ya que Breiman no solo cuestionaba el enfoque dominante, sino que también sugería que la estadística, si no adaptaba su metodología, corría el riesgo de volverse irrelevante en la era de los datos masivos y la informática.\nY lo petó no solo por lo que postulaba en el artículo, sino porque Leo no era un don nadie. Breiman era un peso pesado, un gigante con un historial intachable, reconocido por sus contribuciones fundamentales en la teoría de la probabilidad y el desarrollo de los métodos estadísticos. Antes de convertirse en el “renegado” que abrazó los métodos algorítmicos, ya había dejado su marca como un respetado académico, con trabajos como los árboles de clasificación y regresión (CART), y más tarde con su revolucionario Random Forest.\nSi, Leo era el inventor de Random Forest, que no solo mejoró el análisis de datos, sino que introdujo un enfoque completamente nuevo para manejar la incertidumbre y la variabilidad en la predicción.\nEsto hacía que su crítica fuera doblemente incendiaria: no se trataba de un outsider lanzando piedras desde fuera, sino de uno de los suyos, un miembro prominente de la misma tribu, quien ahora apuntaba a las mismas bases que habían sostenido décadas de desarrollo estadístico. Y lo hacía con el poder de alguien que había creado una de las herramientas más potentes para la predicción: Random Forest, un método que no solo rompía con las estructuras clásicas, sino que las superaba en muchos casos. Así, no era solo lo que decía lo que causó tanto revuelo, sino quién lo decía. Breiman no estaba simplemente abriendo un nuevo camino, estaba sugiriendo que la ruta seguida hasta entonces tal vez ya no llevaba a ninguna parte… o peor aún, que los estadísticos podrían estar caminando en círculos, mientras él, con su Random Forest, ofrecía una brújula algorítmica para avanzar hacia lo desconocido.\nLas respuestas no se hicieron esperar y continúan hasta hoy, con autores como Bradley Efron, David Cox y otros líderes en la estadística frecuentista defendiendo la importancia del modelado basado en teorías sólidas y la atribución causal, mientras reconocían la necesidad de incorporar enfoques más pragmáticos. Este debate continúa siendo relevante, especialmente con el auge del machine learning y la inteligencia artificial."
  },
  {
    "objectID": "posts/deep_work_1/productividad_satisfacción_deep_work.html",
    "href": "posts/deep_work_1/productividad_satisfacción_deep_work.html",
    "title": "Productividad y satisfacción laboral en la Economía del Conocimiento",
    "section": "",
    "text": "El 28 de junio de este año Nike presentó sus resultados financieros del 2º trimestre. Ese mismo día, sus acciones en bolsa cayeron un 20%: perdieron \\(25000\\) millones de dólares en un solo día.\nCon eso acumulaban en 9 meses la friolera de \\(70000\\) millones de dólares perdidos y conseguían el dudoso honor de alcanzar el mínimo en bolsa desde 2018, \\(-32\\)% desde principio de año.\nEn Nike lo llaman “El Día del Juicio”.\n¿Qué había pasado?\n\nUna Empresa data driven"
  }
]